---
title: "Data Workflow Best Practices"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

#### Overview
The workflow of test development is the transformation of raw data into published research results. This data workflow has three components:

1. __Input__: Raw data in .csv or .xlsx files.
2. __Process__: Computer code that operates on the data (i.e., reads the input, transforms and analyzes it, writes the output).
3. __Output__: Tables and graphs that display the results of data analysis, formatted to support print and digital publication.

This document describes the core principles of a _robust_ data workflow based on R, RStudio, and the tidyverse. Going forward, we will use the terms "method", "process", and "workflow" somewhat interchangeably. Here, "robustness" means something like "widely applicable, adaptable, and resilient against external sources of error and disruption." A robust process imposes stringent and specific requirements on it inputs, and allows equally rigorous definition of its outputs. But when the input conditions are satisfied, the robust process, _in its generic form_, consistently and reliably yields the required output.

In practice, this robust workflow can be automated and re-purposed across projects, minimizing start-up time, code customization, copying-and-pasting within scripts, and vulnerability to operator error. The robust process is supported by code templates, detailed documentation, and a set of work habits and guidelines for how data files are managed and shared among multiple users. 

The document contains blocks of executable code, alongside extensive commentary. The narrative is organized into sections reflecting discrete topics and operations in the workflow. Each section has two parts:

1. `EXECUTABLE CODE`: this code can be copy-and-pasted into a working R script. Before running, make sure that all file paths are specified correctly for the local environment. __Note that all code in this document was written for the Mac OS; Windows typically requires the substitution of `\` for `/` in file paths.__
2. `COMMENTED SNIPPETS`: these snippets provide analysis and explanation of the code and workflow. The R code in these snippets is redundant with that in the `EXECUTABLE CODE` section. Here, it serves merely to enhance the documentation, and is not meant to be run.

This HTML document is sourced from (and can be updated with) an accompanying R Notebook.[^longnote]

[^longnote]:The HTML document is generated by an R Notebook markdown document (.rmd). To generate an updated version of the HTML document, render (knit) the Notebook into the RStudio Viewer Pane:

    * In RStudio Preferences, R Markdown section, set the `Show output preview in:` option to `Viewer Pane`.
    * Knit the notebook using `File -> Knit Document`. This will produce a readable HTML version of this document in the Viewer Pane. 
    * Close this Notebook (once you have the HTML in the Viewer, you no longer need the Notebook).
    * Open a new R script in the Source Pane. This will serve as your working script, into which you can copy-and-paste code from the HTML.

<br>

#### First things first: Create an RStudio project and standard folder structure
The robust workflow is based on the RStudio project folder structure. We can set up a project in RStudio by clicking _File -> New Project_. Specify a top-level working directory associated with the project. Create the following folders within the working directory:

* `CODE`: R scripts (`*.R`)
* `DOCUMENTATION`: explanatory documents (`*.rmd`, `*.html`)
* `INPUT-FILES`: data to be processed by R scripts (`*.csv`)
* `OUTPUT-FILES`: saved output from R scripts (`*.csv`, `*.jpg`, `*.png`)

Within these folders, sub-folders will vary by project. There may also be other project-specific folders at the top level.
 
<br>

#### Install and load core packages
###### EXECUTABLE CODE
```{r core, eval = FALSE}
suppressMessages(library(here))
suppressMessages(suppressWarnings(library(tidyverse)))
```
Use `base::install.packages()` and `base::library()` to install and load the core packages for file path specification (`here`) and data wrangling (`tidyverse`). Use `base::suppressMessages()` and `base::suppressWarnings()` as needed to quiet chatter in the console.^[The double-colon `::` operator links a function to its parent package, as in `package::function()`. By using this elaborated specification in code, we can run an installed function without loading its entire parent library. 
Regardless, in code and accompanying commentary, a function name is always followed by a double-parenthesis wrapper `()`.]

`tidyverse` is a meta-package that contains the suite of libraries and functions required for implementing the robust data workflow described in this document.

`here::here()` anchors the file path to the top-level project folder. Thus, other users of the script need only replicate the standard project folder structure on their local machines, and read/write functions will run without error.

The next snippet demonstrates the use of `here()` to read an input file (with `readr::read_csv()`, which is included in the `tidyverse` package):
```
input <- read_csv(here("INPUT-FILES/data.csv"))
```
Note that the file path is enclosed in double-quotes, and it _does not_ begin with a leading `/`.

<br>

#### Robust templates: Tokenization of project-specific elements

###### EXECUTABLE CODE
```{r token, eval = FALSE}
urlRemote_path  <- "https://raw.githubusercontent.com/"
github_path <- "DSHerzberg/RATING-SCALE-ANALYSIS/master/INPUT-FILES/"
input_name <- "data-RS-sim-child-parent.csv"

item_prefix <- "cp"
scale_prefix <- "CP"
scale_suffix <- c("S1", "S2", "S3", "S4", "S5", "TOT")
age_range_name <- "child"
form_name <- "parent"
all_raw_range <- 10:200
TOT_raw_lower_bound <- 50
subscale_raw_upper_bound <- 40
t_score_lower_bound <- 40
t_score_upper_bound <- 80

assign(str_c("data", age_range_name, form_name, sep = "_"),
       suppressMessages(read_csv(url(
         str_c(urlRemote_path, github_path, input_name)
       ))))
```

<br>

###### COMMENTED SNIPPETS

##### Named objects in the R workspace

When R boots up, it opens a workspace, or programming environment, in local memory. The environment contains _objects_, or data elements (e.g., vectors, data frames, lists, etc). Crucially, each object has a unique name that allows it to be referenced by code.

Our robust data workflow operates in this programming environment.^[Actually, many discrete environments may exist within the R workspace. In general our data objects are contained in the most general and inclusive of these: the _global environment_.] The workflow's _process_ component consists of R code that manipulates named data objects. By reading the _input_ components of the workflow, we initialize them as named input objects in the R environment. Thus, the input components now exist in local memory, in addition to being saved files in some other location. Applying code, we transform input objects into named output objects, which also reside in local memory. We complete the process by writing the output objects to external files (saved elsewhere), where they become the _output_ components of the data workflow.

Clearly, names are an important feature of the workflow. R provides naming functions that allow us to assemble and manipulate names to support the robustness of our code. More on this in a moment.

To create code that is coherent, readable and aligned with the general nomenclature of the project, the names of R data objects should either be identical to, or closely related to the names present in the raw data files (the input components of the data workflow).

The names in the raw data files encompass the names of folders, files, variable, and more specifically names of forms, scales, age-ranges, raters, operators, and so on. These names belong to a class of _project-specific_ data elements. Consider the differences between a project to develop an autism parent interview and a project to develop an ADHD behavior rating scale. It's easy to imagine the differences in file and variable names, among many other parameters, between these two projects

In creating code templates to support a robust data workflow, we are faced with a challenge. Our goal is templates that can work for different projects with little or no code customization. But different projects have different file and variable names, and the code has to be set up to handle these different parameter seamlessly. What we need is a way to designate the project-specific names once, and then have those names flow automatically through the code. We also need a way to assemble more complex names that combine several project specific parameters.

Robust code templates make use of a certain class of named objects called _tokens_. A token contains an element of data that is processed one or more times by the code. Examples of tokens include names of variables, files, folders, test forms, raters, etc., as well as labels for age and score ranges, score item counts, numerical bounds, and so forth. 

What these examples all have in common is that they represent _project-specific_ data elements. Thus, for example, variable names typically differ between, say, an autism rating-scale project and an ADHD rating-scale project.

Robust templates can be adapted to different projects with minimal sustitution and copying-and-pasting of text. 

In contrast, the _project-general_ aspects of an R script are those functions and operations whose specification is identical for all projects (e.g., creating a table of descriptive statistics requires calling the same set of functions, regardless of whether the input data are from the autism rating scale or the ADHD rating scale). In a robust template, tokens representing project-specific elements are defined at the head of the script. The remainder of the script consists of project-general code that refers to the project-specific tokens.

Tokens are usually represented in R as _vectors_. The most common way of creating a vector is by using the assignment operator `<-`, which creates a data object in the R programming environment and assigns a name to it. For example, the first line of the next snippet is `urlRemote_path  <- "https://raw.githubusercontent.com/"`, which creates the token `urlRemote_path`, a vector whose sole element is the string `"https://raw.githubusercontent.com/"`. When initializing a vector that contains more than one string element, we wrap the elements in the combine function `base::c()`, as in `scale_suffix <- c("S1", "S2", "S3", "S4", "S5", "TOT")`.

```{r token, echo = 1:15, eval = F}
```
##### Object naming methods

Many analytic procedures require the specification of sets of project-specific elements. For example, we often need to refer to the entire set of item names, or the entire set of form names. To assemble these sets, we need to specify the name of the set, and the names of the elements within the set.

To create the required names, we can a string functions such as `stringr::str_c()` to combine tokens and explicit text. For example,
```
prefix <- "i"
items <- str_c(prefix, c("001", "002", "003"), sep = "_")
```
returns a token (vector) that is a set of item names. The token is named `items`, and the item names are `"i_001" "i_002" "i_003"`.

Often, we need to name an object (create a token) not with an explicit string such as `"items`", but with a concatenated string returned by a function such as `str_c()`. In these instances, instead of the assignment operator `<-`, we use `base::assign()` to name the object (token). `assign()` takes two arguments, the desired token name (concatenated with `str_c()`), and the expression that defines the object(s) to be named.

In the example below, we use `assign()` to name an input data file read in by `readr::read_csv()`. The first argument, `str_c("data", age_range_name, form_name, sep = "_")`, returns the name `data_child_parent`, combining the file type description (`"data"`, provided as an explicit string) with the age-range (`age_range_name`) and rater (`form_name`) tokens. The combination of these latter two tokens identifies the form whose data is contained in the input file. The last argument to `str_c()` is `sep = "_"`, which indicates that in the returned name, the concatenated elements will be separated by underscores.

The second argument to `assign()` specifies the data object to be named with the concatenated string. Here, an object is created by reading the input data file with `read_csv()`. Note how the input file path is a concatenation of three previously defined tokens `str_c(urlRemote_path, github_path, input_name)`.
```{r token, echo = 16:19, eval = F}
```

<br>

#### Input requirements: File-naming conventions (work in progress)

File name example:

```
"tod-data-stand-amk.csv"
```


The robust data workflow requires standardization of the names of .csv and .xlsx files that contain raw data. Some ground rules:

* Within R code, references to external file paths and saved files are always enclosed in double quotes `""`
* File names SHOULD NOT contain:
    + blank spaces (replace with hyphens `-`)
    + any other non-alphanumeri characters (besides hyphens)
    + upper-case letters
* Names of persons should be expressed as lower-case three-letter acronyms
    
Thus, we use `"jfd-project-data.csv"`, and NOT `John Doe's Project Data.csv`.

When reading files into R, we may need to take into account folder and file structures set up to facilitate manual entry of data from printed test forms. When multiple operators are working on the same project, they may save their work in distinct MS Excel files, on their local machines or in their own folders on the organization's network.

Each operator may then create several different .xlsx files to differentiate (for example) data from clinical cases, typically developing children, a test-retest cohort, and so on. And, within an .xlsx file, there may be multiple worksheets (tabs) to subdivide data within cases. For example, there may be separate sheets for demographic information, responses from the test under development, responses from concurrently administered measures, and so on.

What's important to note is that each of these different ways of dividing up the data (which are implemented to facilite data entry) represent information that we typically want to preserve and make available for analysis. For example, we might want to compare the data entered by different operators. To do this, we would need to create a variable (column) where each case would receive a code indicating the operator who entered that case. But the information required to assign values in that column many not be represented explicitly in the .xlsx sheets themselves. In fact, it may be represented in a folder structure, which is not a format that we can analyze with a statistical package.

Here is where the file naming, in combination with tokenization, becomes useful. We can represent two levels of differentiation in the file name. Suppose we have two operators, John Frederick Doe, and Jane Maria Roe; and two data sources, typical and clinical. We can tokenize these variables as follows:

```
operator <- c("jfd", "jmr")
data_source <- c("clinical", "typical")
```

And we would name data files accordingly:

```
"strat1-strat2-project_name-data.csv"
```

<br>

#### Input requirements: Variable-naming conventions (work in progress)

<br>

#### Date-Time formats
